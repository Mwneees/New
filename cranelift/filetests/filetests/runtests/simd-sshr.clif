test run
set enable_simd
target aarch64
; target s390x
target x86_64 skylake


function %sshr_i8x16() -> b1 {
block0:
    v0 = iconst.i32 1
    v1 = vconst.i8x16 [0 0xff 2 0xfd 4 0xfb 6 0xf9 8 0xf7 10 0xf5 12 0xf3 14 0xf1]
    v2 = sshr v1, v0

    v3 = vconst.i8x16 [0 0xff 1 0xfe 2 0xfd 3 0xfc 4 0xfb 5 0xfa 6 0xf9 7 0xf8]
    v4 = icmp eq v2, v3
    v5 = vall_true v4
    return v5
}
; run

function %sshr_i16x8() -> b1 {
block0:
    v0 = iconst.i32 1
    v1 = vconst.i16x8 [-1 2 4 8 -16 32 64 128]
    v2 = sshr v1, v0

    v3 = extractlane v2, 0
    v4 = icmp_imm eq v3, 0xffff ; because of the shifted-in sign-bit, this remains 0xffff == -1

    v5 = extractlane v2, 4
    v6 = icmp_imm eq v5, 0xfff8 ; -16 has been shifted to -8 == 0xfff8

    v7 = band v4, v6
    return v7
}
; run

function %sshr_too_large_i32x4() -> b1 {
block0:
    v0 = iconst.i32 33 ; note that this will wrap, being equivalent to shifting by 1
    v1 = vconst.i32x4 [1 2 4 -8]
    v2 = sshr v1, v0

    v3 = extractlane v2, 0
    v4 = icmp_imm eq v3, 0

    v5 = extractlane v2, 3
    v6 = icmp_imm eq v5, 0xffff_ffff ; shifting in the sign-bit repeatedly fills the result with 1s

    v7 = band v4, v6
    return v7
}
; run

function %sshr_i64x2(i64x2, i32) -> i64x2 {
block0(v0:i64x2, v1:i32):
    v2 = sshr v0, v1
    return v2
}
; run: %sshr_i64x2([1 -1], 0) == [1 -1]
; run: %sshr_i64x2([1 -1], 1) == [0 -1] ; note the -1 shift result
; run: %sshr_i64x2([2 -2], 1) == [1 -1]
; run: %sshr_i64x2([0x80000000_00000000 0x7FFFFFFF_FFFFFFFF], 63) == [0xFFFFFFFF_FFFFFFFF 0]

function %sshr_imm_i32x4() -> b1 {
block0:
    v1 = vconst.i32x4 [1 2 4 -8]
    v2 = sshr_imm v1, 1

    v3 = vconst.i32x4 [0 1 2 -4]
    v4 = icmp eq v2, v3
    v5 = vall_true v4
    return v5
}
; run

function %sshr_i8x16(i8x16, i32) -> i8x16 {
block0(v0: i8x16, v1: i32):
    v2 = sshr v0, v1
    return v2
}
; run: %sshr_i8x16([0 0xff 2 0xfd 4 0xfb 6 0xf9 8 0xf7 10 0xf5 12 0xf3 14 0xf1], 1) == [0 0xff 1 0xfe 2 0xfd 3 0xfc 4 0xfb 5 0xfa 6 0xf9 7 0xf8]

function %sshr_i16x8(i16x8, i32) -> i16x8 {
block0(v0: i16x8, v1: i32):
    v2 = sshr v0, v1
    return v2
}
; note: because of the shifted-in sign-bit, lane 0 remains -1 == 0xffff, whereas lane 4 has been shifted to -8 == 0xfff8
; run: %ushr_i16x8([-1 2 4 8 -16 32 64 128], 1) == [-1 1 2 4 -8 16 32 64]

function %sshr_i32x4(i32x4, i32) -> i32x4 {
block0(v0: i32x4, v1: i32):
    v2 = sshr v0, v1
    return v2
}
; note: shifting in the sign-bit repeatedly in lane 3 fills the result with 1s (-1 == 0xffff_ffff)
; run: %ushr_i32x4([1 2 4 -8], 33) == [0 0 0 0xffff_ffff]

function %sshr_i64x2(i64x2, i32) -> i64x2 {
block0(v0:i64x2, v1:i32):
    v2 = sshr v0, v1
    return v2
}
; run: %sshr_i64x2([1 -1], 0) == [1 -1]
; run: %sshr_i64x2([1 -1], 1) == [0 -1] ; note the -1 shift result
; run: %sshr_i64x2([2 -2], 1) == [1 -1]
; run: %sshr_i64x2([0x80000000_00000000 0x7FFFFFFF_FFFFFFFF], 63) == [0xFFFFFFFF_FFFFFFFF 0]

function %sshr_imm_i32x4(i32x4) -> i32x4 {
block0(v0: i32x4):
    v1 = sshr_imm v0, 1
    return v1
}
; run: %sshr_imm_i32x4([1 2 4 -8]) == [0 1 2 -4]

function %sshr_imm_i16x8(i16x8) -> i16x8 {
block0(v0: i16x8):
    v1 = sshr_imm v0, 1
    return v1
}
; run: %sshr_imm_i16x8([1 2 4 -8 0 0 0 0]) == [0 1 2 -4 0 0 0 0]
