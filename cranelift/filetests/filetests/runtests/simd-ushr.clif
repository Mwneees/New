test run
set enable_simd
target aarch64
; target s390x
target x86_64 skylake


function %ushr_i8x16() -> b1 {
block0:
    v0 = iconst.i32 1
    v1 = vconst.i8x16 [0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15]
    v2 = ushr v1, v0

    v3 = vconst.i8x16 [0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7]
    v4 = icmp eq v2, v3
    v5 = vall_true v4
    return v5
}
; run


function %ushr_i64x2() -> b1 {
block0:
    v0 = iconst.i32 1
    v1 = vconst.i64x2 [1 2]
    v2 = ushr v1, v0

    v3 = extractlane v2, 0
    v4 = icmp_imm eq v3, 0

    v5 = extractlane v2, 1
    v6 = icmp_imm eq v5, 1

    v7 = band v4, v6
    return v7
}
; run


function %ushr_too_large_i32x4() -> b1 {
block0:
    v0 = iconst.i32 33 ; note that this will wrap, being equivalent to shifting by 1
    v1 = vconst.i32x4 [1 2 4 8]
    v2 = ushr v1, v0

    v3 = extractlane v2, 0
    v4 = icmp_imm eq v3, 0

    v5 = extractlane v2, 3
    v6 = icmp_imm eq v5, 0

    v7 = band v4, v6
    return v7
}
; run


function %sshr_imm_i16x8() -> b1 {
block0:
    v1 = vconst.i16x8 [1 2 4 -8 0 0 0 0]
    v2 = ushr_imm v1, 1

    v3 = vconst.i16x8 [0 1 2 32764 0 0 0 0] ; -4 with MSB unset == 32764
    v4 = icmp eq v2, v3
    v5 = vall_true v4
    return v5
}
; run


function %ushr_i8x16(i8x16, i32) -> i8x16 {
block0(v0: i8x16, v1: i32):
    v2 = ushr v0, v1
    return v2
}
; run: %ushr_i8x16([0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15], 1) == [0 0 1 1 2 2 3 3 4 4 5 5 6 6 7 7]

function %ushr_i32x4(i32x4, i32) -> i32x4 {
block0(v0: i32x4, v1: i32):
    v2 = ushr v0, v1
    return v2
}
; run: %ushr_i32x4([1 2 4 8], 33) == [0 0 0 0]

function %ushr_i64x2(i64x2, i32) -> i64x2 {
block0(v0: i64x2, v1: i32):
    v2 = ushr v0, v1
    return v2
}
; run: %ushr_i64x2([1 2], 1) == [0 1]
